
# Semantic Mapping Framework
- Github repo : https://github.com/Chialiang86/Semantic-Mapping-Framework

## Data Collection
- collect data by data_generator.py
- collect 200 frames in each room of each scene

#### Other Scenes : 2600 frames
- apartment_1 : 400 frames
- apartment_2 : 800 frames
- hotel_0 : 1200 frames
- office_0 : 200 frames

#### Apartment_0 : 2600 frames
- 200 frames in each room

## Training scemantic map
- hyperparameters

```yaml
TRAIN:
  batch_size_per_gpu: 2
  beta1: 0.9
  deep_sup_scale: 0.4
  disp_iter: 50
  epoch_iters: 1300
  fix_bn: False
  lr_decoder: 0.02
  lr_encoder: 0.02
  lr_pow: 0.9
  num_epoch: 20
  optim: SGD
  seed: 304
  start_epoch: 0
  weight_decay: 0.0001
  workers: 8
```

## 3d_semantic_map.py details
execution : ```python 3d_semantic_map.py --floor [1, 2]```
> the floor argument is the floor num of apartment_0 scene
1. Read all the depth image and semantic maps for 
    - I : ground truth semantic maps
    - II : semantic maps generated by models trained with other scenes
    - III : semantic maps generated bt models trained with apartment_0
2. Convert depth images and semantic maps to point cloud
3. Down sample the point clouds by the custom voxel_down_sample 

## Voxel downsample function implement details
- Strategy : round the point cloud by deviding each point by voxel size (ex: 0.05), and choose the nearest neighbor's color of the matched rounded point as representative color 
```python 
def voxel_down_sample(pcd, voxel_size):
    
    pts = np.asarray(pcd.points)
    clrs = np.asarray(pcd.colors)

    # round by voxel_size, ex : 0.05
    pts_round = np.round(np.array(pts / voxel_size)).astype(int)
    # remove redundant data
    pts_down = np.unique(pts_round, axis=0) *  voxel_size

    # find the index of nearset neighbor of the rounded 3d points
    nn = NearestNeighbors(n_neighbors=1)
    nn.fit(pts)
    _, idx = nn.kneighbors(pts_down)
    clrs_down = clrs[idx]
    clrs_down = np.reshape(clrs_down, (clrs_down.shape[0], clrs_down.shape[2]))

    # to point cloud
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(pts_down)
    pcd.colors = o3d.utility.Vector3dVector(clrs_down)

    return pcd
```

## Result

#### Training and validation log

- Training result : 
![](https://i.imgur.com/0xF1mfv.png)

- Validation result : 
    - first floor : other scenes
    ![](https://i.imgur.com/U3XGQVH.png)

    - first floor : apartment_0
    ![](https://i.imgur.com/x1qByom.png)

    - second floor : other_scenes
    ![](https://i.imgur.com/ukJ6VhL.png)

    - second floor : apartment_0
    ![](https://i.imgur.com/vZJxijX.png)
    
I don't know why the result was so bad :(

#### 3D reconstruction

1. ground truth

first floor
![](https://i.imgur.com/h1mlhVh.png)
second floor
![](https://i.imgur.com/7WPO8XH.png)

2. predicted by the model trained by other scenes

first floor
![](https://i.imgur.com/ISMA9vO.png)
second floor
![](https://i.imgur.com/L8nw2lX.png)

3. predicted by the model trained by apartment_0

first floor
![](https://i.imgur.com/oUzMj96.png)
second floor
![](https://i.imgur.com/M2G1RkU.png)

